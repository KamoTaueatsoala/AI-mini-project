# -*- coding: utf-8 -*-
"""Processing_code.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Dtsrv4S20Vl2G3rnWKYeYyVfrpATgdas
"""

# Import libraries: These are tools we need to work with images, data, and machine learning
import cv2  # Helps process images (like resizing or filtering)
import numpy as np  # Used for math and handling arrays (lists of numbers)
import pandas as pd  # Helps organize data into tables (like a spreadsheet)
import os  # Lets us work with files and folders
from sklearn.model_selection import train_test_split  # Splits data into training and testing sets
from tensorflow.keras.preprocessing.image import ImageDataGenerator  # Adds variety to images for better training
from PIL import Image  # Another tool for working with images
import matplotlib.pyplot as plt  # Used to show images or plots

# Mount Google Drive: Connects to Google Drive to access the dataset folder
from google.colab import drive
drive.mount('/content/drive')  # Links Google Drive to this program (Colab is a cloud tool)

# Set paths: Tells the program where to find the images and info file
data_dir = '/content/drive/MyDrive/all-mias'  # Folder with all mammogram images
info_path = '/content/drive/MyDrive/all-mias/Info.txt'  # File with details about each image

# Debug: Check paths: Prints to make sure the folder and files are found
print(f"Data directory: {data_dir}")  # Shows the folder path
print(f"Info path: {info_path}")  # Shows the path to Info.txt
print(f"Files in data_dir: {os.listdir(data_dir)}")  # Lists all files in the folder

# Get unique PGM file references: Finds all image files (.pgm) and lists their names without ".pgm"
pgm_files = sorted([f.replace('.pgm', '') for f in os.listdir(data_dir) if f.endswith('.pgm')])  # Gets image names (e.g., mdb001)
print(f"Unique PGM file references: {len(pgm_files)} - {pgm_files}")  # Shows how many images and their names

# Inspect Info.txt: Shows the first 3000 characters of Info.txt to check its contents
print("First 3000 characters of Info.txt:")
with open(info_path, 'r') as f:  # Opens Info.txt for reading
    content = f.read()[:3000]  # Reads first 3000 characters
    print(content)  # Prints them to see what’s inside

# Load and parse Info.txt: Turns Info.txt into a table for easy use
columns = ['ref', 'bg', 'abnorm_class', 'severity', 'x', 'y', 'radius']  # Defines column names for the table
data = []  # Empty list to store data
start_data = False  # Flag to know when to start reading data
with open(info_path, 'r') as f:  # Opens Info.txt again
    for line in f:  # Reads each line
        line = line.strip()  # Removes extra spaces
        if 'INFORMATION:' in line:  # Looks for the part where data starts
            start_data = True  # Signals that data comes next
            continue
        if start_data and line and line.startswith('mdb'):  # If in data section and line is an image entry
            if len(data) >= 322:  # Stops after 322 entries (total images)
                break
            parts = line.split()  # Splits line into pieces (e.g., ref, bg, etc.)
            if len(parts) >= 3:  # Makes sure there’s enough info
                if len(parts) < 7:  # If too few pieces, adds blank values
                    parts.extend([np.nan] * (7 - len(parts)))
                elif len(parts) > 7:  # If too many pieces, keeps only first 7
                    parts = parts[:7]
                data.append(parts[:7])  # Adds the row to data list

df = pd.DataFrame(data, columns=columns)  # Creates a table (DataFrame) with the data
print("Parsed DataFrame:")  # Shows the table
print(df.head())  # Prints first few rows to check
print(f"Total rows in DataFrame: {len(df)}")  # Shows how many rows (should be 322)

# Filter to match exact PGM files: Keeps only rows that match image files
df = df[df['ref'].isin(pgm_files)]  # Removes any rows where the image file is missing
print(f"Filtered DataFrame rows (matching PGM files): {len(df)}")  # Shows how many rows remain

# Map labels: Converts severity (Normal, Benign, Malignant) to numbers for machine learning
df['label'] = df['severity'].map({np.nan: 0, 'B': 1, 'M': 2})  # 0 = Normal, 1 = Benign, 2 = Malignant
df = df.dropna(subset=['ref'])  # Removes any rows with missing image names

# Preprocessing function: Prepares each image for machine learning
def preprocess_image(file_path, target_size=(224, 224)):  # Takes image path and desired size
    img = cv2.imread(file_path, cv2.IMREAD_GRAYSCALE)  # Loads image in grayscale
    if img is None:  # Checks if image loaded correctly
        print(f"Failed to load {file_path}")  # Warns if image is missing
        return None
    img = cv2.medianBlur(img, 5)  # Smooths the image to reduce noise
    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))  # Enhances contrast for better details
    img = clahe.apply(img)  # Applies contrast enhancement
    img = cv2.resize(img, target_size, interpolation=cv2.INTER_AREA)  # Resizes to 224x224 pixels
    img = img / 255.0  # Scales pixel values to 0-1 for machine learning
    img = cv2.cvtColor(img.astype(np.float32), cv2.COLOR_GRAY2RGB)  # Converts grayscale to RGB (3 channels)
    return img  # Returns the processed image

# New function: Save processed images and labels to Google Drive
def save_processed_data(images, labels, save_dir='/content/drive/MyDrive/all-mias'):
    # Make sure the save directory exists
    if not os.path.exists(save_dir):
        os.makedirs(save_dir)  # Creates the folder if it doesn't exist
    # Save images and labels as .npy files
    np.save(os.path.join(save_dir, 'processed_images.npy'), images)  # Saves images
    np.save(os.path.join(save_dir, 'processed_labels.npy'), labels)  # Saves labels
    print(f"Saved processed images and labels to {save_dir}")  # Confirms saving

# New function: Load processed images and labels from Google Drive
def load_processed_data(save_dir='/content/drive/MyDrive/all-mias'):
    # Check if saved files exist
    images_path = os.path.join(save_dir, 'processed_images.npy')
    labels_path = os.path.join(save_dir, 'processed_labels.npy')
    if os.path.exists(images_path) and os.path.exists(labels_path):
        images = np.load(images_path)  # Loads images
        labels = np.load(labels_path)  # Loads labels
        print(f"Loaded {len(images)} images and {len(labels)} labels from {save_dir}")
        return images, labels
    else:
        print(f"No saved data found in {save_dir}. Run preprocessing first.")
        return None, None

# Load images and labels: Loads all images and their labels
images = []  # List to store processed images
labels = []  # List to store labels (0, 1, or 2)
for idx, row in df.iterrows():  # Loops through each row in the table
    img_path = os.path.join(data_dir, row['ref'] + '.pgm')  # Builds path to image file
    if os.path.exists(img_path):  # Checks if image file exists
        img = preprocess_image(img_path)  # Processes the image
        if img is not None:  # If image processed successfully
            images.append(img)  # Adds image to list
            labels.append(row['label'])  # Adds label to list
    else:
        print(f"Image not found: {img_path}")  # Warns if image is missing

images = np.array(images)  # Converts image list to an array for machine learning
labels = np.array(labels)  # Converts label list to an array
if len(images) == 0:  # Checks if any images were loaded
    print("No images loaded. Check data_dir and file extensions.")  # Warns if no images loaded
else:
    print(f"Loaded {len(images)} images and {len(labels)} labels")  # Confirms how many images and labels loaded

# Save the processed data: Calls the new save function
save_processed_data(images, labels)  # Saves images and labels to Google Drive

# Train/test split: Splits data into training (80%) and testing (20%) sets
X_train, X_test, y_train, y_test = train_test_split(images, labels, test_size=0.2, stratify=labels, random_state=42)
# X_train: Training images, X_test: Testing images, y_train: Training labels, y_test: Testing labels
# test_size=0.2: 20% for testing, 80% for training
# stratify=labels: Keeps the same proportion of labels (0, 1, 2) in both sets
# random_state=42: Makes the split consistent every time

# Data augmentation: Adds variety to training images to improve model learning
datagen = ImageDataGenerator(rotation_range=20, width_shift_range=0.2, height_shift_range=0.2, shear_range=0.2, zoom_range=0.2, horizontal_flip=True, fill_mode='nearest')
# Rotates, shifts, zooms, and flips images slightly to create new versions
datagen.fit(X_train)  # Prepares the augmentation for training images

# Visualize: Shows one training image to check it
plt.imshow(X_train[0])  # Displays the first training image
plt.title(f"Label: {y_train[0]} (0=Normal, 1=Benign, 2=Malignant)")  # Shows the label (0, 1, or 2)
plt.show()  # Opens a window to view the image